{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/metricsTab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "*“How often is the classifier correct?”*\n",
    "\n",
    "![](img/Accuracy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "*\"The confusion matrix is another metric that is often used to measure the performance of a classification algorithm. True to its name, the terminology related to the confusion matrix can be rather confusing, but the matrix itself is simple to understand.\"*\n",
    "\n",
    "![Confusion Matrix](img/ConfusionMatrix.png)\n",
    "\n",
    "![Confusion Matrix](img/ConfusionMatrix2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, recall and f1-score\n",
    "\n",
    "**Precision** : it answers the question:\n",
    "\n",
    "*“When it predicts the positive result, how often is it correct?”*\n",
    "\n",
    "![Precision](img/Precision.png)\n",
    "\n",
    "**Recall** : it answers the question:\n",
    "\n",
    "*“When it is actually the positive result, how often does it predict correctly?”*\n",
    "\n",
    "![Recall](img/Recall.png)\n",
    "\n",
    "**f1-score** : this is just the harmonic mean of precision and recall\n",
    "\n",
    "![f1-score](img/f1-score.png)\n",
    "\n",
    "It is useful when you need to take both precision and recall into account. If you try to only optimize recall, your algorithm will predict most examples to belong to the positive class, but that will result in many false positives and, hence, low precision. On the other hand, if you try to optimize precision, your model will predict very few examples as positive results (the ones which highest probability), but recall will be very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve \n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "# create plot\n",
    "plt.plot(precision, recall, label='Precision-recall curve')\n",
    "_ = plt.xlabel('Precision')\n",
    "_ = plt.ylabel('Recall')\n",
    "_ = plt.title('Precision-recall curve')\n",
    "_ = plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "\n",
    "*“When it is actually the negative result, how often does it predict incorrectly?”*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "# create plot\n",
    "plt.plot(fpr, tpr, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random guess')\n",
    "_ = plt.xlabel('False Positive Rate')\n",
    "_ = plt.ylabel('True Positive Rate')\n",
    "_ = plt.title('ROC Curve')\n",
    "_ = plt.xlim([-0.02, 1])\n",
    "_ = plt.ylim([0, 1.02])\n",
    "_ = plt.legend(loc=\"lower right\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Medicision",
   "language": "python",
   "name": "medicision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
